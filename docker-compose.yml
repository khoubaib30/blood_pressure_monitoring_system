services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - kafka-network
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_started
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - kafka-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9093 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    container_name: blood-pressure-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9093
      KAFKA_TOPIC: ${KAFKA_TOPIC:-blood-pressure-data}
      PYTHONUNBUFFERED: 0
    volumes:
      - ./src:/app/src
    networks:
      - kafka-network
    command: python src/producer.py
    restart: unless-stopped

  consumer:
    build:
      context: .
      dockerfile: Dockerfile.consumer
    container_name: blood-pressure-consumer
    depends_on:
      kafka:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9093
      KAFKA_TOPIC: ${KAFKA_TOPIC:-blood-pressure-data}
      ELASTICSEARCH_HOST: elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_INDEX: ${ELASTICSEARCH_INDEX:-blood-pressure-observations}
      PYTHONUNBUFFERED: 0
    volumes:
      - ./src:/app/src
      - ./data:/app/data
    networks:
      - kafka-network
    command: python src/consumer.py
    restart: unless-stopped

  monitoring:
    build:
      context: .
      dockerfile: Dockerfile.monitoring
    container_name: blood-pressure-monitoring
    depends_on:
      kafka:
        condition: service_healthy
      consumer:
        condition: service_started
    ports:
      - "${FLASK_PORT:-5000}:5000"
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9093
      KAFKA_TOPIC: ${KAFKA_TOPIC:-blood-pressure-data}
      PYTHONUNBUFFERED: 0
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./models:/app/models
    networks:
      - kafka-network
    command: python src/monitoring_dashboard.py

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - bootstrap.memory_lock=false
    ports:
      - "${ELASTICSEARCH_PORT:-9200}:9200"
      - "9300:9300"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    depends_on:
      elasticsearch:
        condition: service_healthy
    ports:
      - "${KIBANA_PORT:-5601}:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=
      - xpack.security.enabled=false
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    restart: unless-stopped

  kibana_init:
    image: curlimages/curl:8.6.0
    container_name: kibana-init
    depends_on:
      kibana:
        condition: service_healthy
    volumes:
      - ./kibana:/kibana:ro
    networks:
      - kafka-network
    restart: "no"
    command: >
      sh -c "
        if [ -s /kibana/saved_objects.ndjson ]; then
          echo 'Importing Kibana saved objects...';
          curl -sS -X POST 'http://kibana:5601/api/saved_objects/_import?overwrite=true'
            -H 'kbn-xsrf: true'
            --form file=@/kibana/saved_objects.ndjson;
          echo 'Kibana saved objects import done.';
        else
          echo 'No kibana/saved_objects.ndjson found or file is empty. Skipping import.';
        fi
      "

networks:
  kafka-network:
    driver: bridge

volumes:
  elasticsearch-data:
